---
title: "\\vspace{2in} MovieLens Project"
subtitle: "Harvard PH125.9x Data Science: Capstone"
author: "Frankie Inguanez"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: yes
    toc_depth: 3
    fig_caption: yes
fontsize: 11pt
include-before: '`\newpage{}`{=latex}'
urlcolor: blue

header-includes:
  \usepackage[nottoc]{tocbibind}
  \pagenumbering{gobble}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.align="center", out.width="80%")

# Global Variables
options(digits = 5)

# Load needed libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggrepel)) install.packages("ggrepel", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("tibble", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(ggrepel)
library(knitr)
library(tibble)

# Load presaved data
load("movieLens.RData")

```

\listoffigures

\newpage
\pagenumbering{arabic}

# **Introduction** 

This report documents the research undertaken for the MovieLens project submission in part fulfillment of the Harvard PH125.9x Data Science: Capstone module by the author, Frankie Inguanez. The [MovieLens](https://movielens.org) dataset is prepared by [GroupLens Research](https://grouplens.org/) at the University of Minnesota which contains user reviews on movies. For this project a subset of the entire dataset has been utilized.

The aim of this research is to propose a movie recommender system. The fitness of the proposed model is evaluated using the Root Mean Square Error (RMSE) and the objective of this research is to have a RMSE lower than 0.86490.

This document proceeds with an overview of the data analysis undertaken, highlighting the data cleaning decisions and illustrating the data visualisations which justify the methodology undertaken in model creation. All findings are presented in the Results section, with final remarks found in conclusion. 

```{r initial-setup, eval=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

\newpage

# **Analysis**
The analysis process is made up of the following stages: Exploratory Analysis; Data cleaning; Data Visualisation; Data Modelling.

The initial setup code provided downloaded the 10 million dataset and split it in a 90:10 ratio for training and testing respectively, resulting in `r format(nrow(edx),big.mark=",",scientific=F)` observations (rows) for training, with `r format(nrow(validation),big.mark=",",scientific=F)` observations (rows). The dataset has `r format(ncol(validation),big.mark=",",scientific=F)` variables(columns).

## Exploratory Analysis
```{r dataExploration, eval=FALSE}
# Checking data types and dimensions
class(edx)
dim(edx)[1]
dim(validation)[1]

dim(edx)[2]
str(edx)

# Checking unique values for user id, movie ids, titles, genres
n_distinct(edx$userId)
n_distinct(edx$movieId) 
n_distinct(edx$title) 

# Identified two movies with same title and release year, investigating further
duplicateMovieTitle <- edx %>% select(movieId, title) %>% unique() %>% 
  group_by(title) %>% 
  summarize(n=n()) %>%
  filter(n>1)

edx %>% filter(title==duplicateMovieTitle$title) %>%
  select(movieId, genres) %>%
  group_by(movieId, genres) %>% 
  summarize(n=n()) %>%
  unique()

# This is not an error but an actual situation.
#https://www.imdb.com/title/tt0407304
#https://www.imdb.com/title/tt0449040

# Extracting all unique genres
genres <- str_extract_all(unique(edx$genres), "[^|]+") %>%
  unlist() %>%
  unique()

n_distinct(genres) # Unique genres
n_distinct(edx$genres) # Unique genres combinations
```

The next step was to explore the data by checking the datatypes, counts and presence of null (NA) values. No missing values were found, the identification variables for users and movies are integer and numeric respectively. Rating is numeric whilst title and genres are character variables. It was noted that genres is a multi-valued using the pipe (|) as a separator. The timestamp variable represents the review date in milliseconds since the 1st January 1970 GMT (epoch time).

Checking the count of each variable resulted in a discrepency with `r format(n_distinct(edx$movieId), big.mark=",", scientific=F)` unique movie ID values but `r format(n_distinct(edx$title), big.mark=",", scientific=F)` unique movie title values. The movie in question is `r duplicateMovieTitle$title[1]`. Upon further investigation this was found to be a genuine case where in the year 2005 two movies were released with the same name, one [directed by Steven Spielberg](https://www.imdb.com/title/tt0407304), whilst the other [directed by David Michael Latt](https://www.imdb.com/title/tt0449040). So it was determined that no data cleaning is needed, yet it is important to use the movieID variable rather than title variable when aggregating by movie.

## Data Cleaning
```{r dataCleaning, eval=FALSE}
# Convert timestamp to date format
edx <- edx %>% mutate(reviewDate = round_date(as_datetime(timestamp), unit = "day"))

# Extraction of movie release year
edx <- edx %>% mutate(title = str_trim(title)) %>%
  extract(title, c("shortTitle", "year"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = FALSE) %>%
  mutate(year = as.integer(year)) %>%
  select(-shortTitle)

# Extraction of distance freom review date to movie releae year
edx <- edx %>%
  mutate(reviewDelay = year(reviewDate)-year)
```

The data cleaning consisted in three main tasks. First the timestamp variable was converted to a human readable date without the time information. This variable was rounded to the nearest day and named reviewDate.

The next task was to separate the release year from the movie title. A regular expression pattern was created to search at the end of the title variable for a space followed by digits in parenthesis. The year variable was also converted to integer.

The last task involved the use of the prior two tasks. In this research we are assuming that there is no real intrinsic value in the review date but rather the difference in years from the movie release year to the actual review date. So a variable called reviewDelay was created to measure this difference.

## Data Visualisation

### Rating

The rating variable ranges from 0 till 5 with 0.5 increments. The whole numbers occur more frequently then the half point ratings. 

```{r ratingsDistribution, fig.cap="Ratings distribution"}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth=0.5, color=I("black")) +
  scale_y_continuous() +
  labs(title = "Ratings distribution", x = "Rating value", y = "Count")
```

### Movies
Looking at the mean rating by movie we see that it is at around 3.5.

```{r movieMeanRating, fig.cap="Movie count by mean rating"}
edx %>% group_by(movieId) %>%
  summarise(mean_rating = mean(rating)) %>%
  ggplot(aes(mean_rating)) +
  geom_histogram(bins=50, color=I("black")) +
  labs(x = "Mean rating", y = "Movie count")
```

### Users
A similar yet more condensed observation is made when looking at the mean rating by user in Figure \@ref(fig:userMeanRating). It is also noted from Figure \@ref(fig:userRatingCount) that certain users rate much more than other users.

```{r userMeanRating, fig.cap="User count by mean rating"}
edx %>% group_by(userId) %>%
  summarise(mean_rating = mean(rating)) %>%
  ggplot(aes(mean_rating)) +
  geom_histogram(bins=50, color = I("black")) +
  labs(x = "Mean rating", y = "User count")
```

```{r userRatingCount, fig.cap="User rating count"}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=50, color = I("black")) +
  scale_x_log10() +
  labs(x = "User", y = "Ratings")
```

### Genre
In observing the mean rating per genre some data manipulation was needed to extract the average rating per genre rather than per genre combination. The generation of such figure was very computationally intensive and the benefit of averaging by genre rather than genre combination is to be determined. Depending on whether the project objective is achieved a decision shall be made on which variable to use. From Figure \@ref(fig:genreMeanRating) we can observe that the average rating does fluctuate a lot per genre, namely for Horror and Film-Noir. The list of genres with ratings count and mean rating is also provided in Table \@ref(tab:genreListing).

```{r genreListing}
genres_df %>% select(genre, n, meanRating)%>%
  kable(col.names = c("Genre", "Ratings Count", "Mean Rating"),
        caption = "Ranked genres by ratings count",
        align = "lrr", booktabs = TRUE, format = "latex", linesep = "")
```

```{r genreMeanRating, fig.cap="Mean rating by genre"}
genres_df %>% filter(genre!="(no genres listed)") %>%
  mutate(genre = reorder(genre, meanRating)) %>%
  ggplot(aes(x = genre, y = meanRating, ymin=meanRating - 2*se, ymax=meanRating + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Genre", y = "Average Rating")
```

### Release Year
From Figure \@ref(fig:yearMeanRating) we can observe a fluctuation in the mean average rating for the release year of the movie. This could be due to the disproportionate number of movies available per release year as shown in Figure \@ref(fig:yearRatingCount).

```{r yearMeanRating, fig.cap="Year mean rating"}
edx %>% group_by(year) %>%
  summarise(rating = mean(rating)) %>%
  ggplot(aes(year, rating)) +
  geom_point() +
  geom_smooth(formula='y~x', method='loess', span = 0.15) +
  labs(x = "Release Year", y = "Average Rating")
```

```{r yearRatingCount, fig.cap="Year rating count"}
edx %>% group_by(year) %>%
  summarise(n = n()) %>%
  ggplot(aes(year, n)) +
  geom_point() +
  scale_y_log10() +
  labs(x = "Release Year", y = "Rating Count")
```

### Review Date & Delay

When observing the mean rating per review date as shown in Figure \@ref(fig:reviewDateMeanRating) no real fluctuation could be observed. It was noted that there was a strange range of values in the review delay, which is the number of years of the review date from the movie release year as shown in Figure \@ref(fig:reviewDelayCount). Some reviews had a negative value, which follow some research was established to be a pre-release screening of a movie for critics which is common practice. It was noted that the number of reviews on a movie is inversely proportional to the review delay. From Figure \@ref(fig:reviewDelayMeanRating) it can be observed that the mean rating does fluctuate by review delay, more specifically the mean rating in the first few years is slightly higher than the subsequent 5-10 years. The mean rating then rises drastically following 10 years of release. Also the mean rating prior to release thens to be considerably lower. With these observations a decision has been made to use review delay instead of review date in the data modelling phase.

```{r reviewDateMeanRating, fig.cap="Review date mean rating"}
edx %>% group_by(reviewDate) %>%
  summarize(mean_rating = mean(rating)) %>%
  ggplot(aes(reviewDate, mean_rating)) +
  geom_point() +
  geom_smooth(formula='y~x', method='loess', span = 0.15) +
  labs(x = "Review Date", y = "Average Rating")
```

```{r reviewDelayCount, fig.cap="Review delay rating count"}
edx %>% group_by(reviewDelay) %>%
  summarise(n = n()) %>%
  ggplot(aes(reviewDelay, n)) +
  geom_point() +
  scale_y_log10()+
  labs(x = "Review Delay", y = "Rating Count")
```

```{r reviewDelayMeanRating, fig.cap="Review delay mean rating"}
edx %>% group_by(reviewDelay) %>%
  summarise(mean_rating = mean(rating)) %>%
  ggplot(aes(reviewDelay, mean_rating)) +
  geom_point() +
  labs(x = "Review Delay", y = "Average Rating")
```

## Data Modelling

```{r dataModelling, eval=FALSE}
# Evaluation metric: Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2, na.rm=TRUE))
}

# Split data in 80:20 train:test ratios
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
d.index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
d.train <- edx[-d.index]
d.test <- edx[d.index]

# Clean-up
rm(d.index)

# Calculate the overall average rating across all movies included in train set
mu_hat <- mean(d.train$rating)

# Model 01: Simple recommender based on mean.
rmse.simple <- RMSE(d.test$rating, mu_hat)

# Model 02: Adding movie effect (b_i)
avg.movies <- d.train %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu_hat))

predicted.b_i <- mu_hat + d.test %>%
  left_join(avg.movies, by = "movieId") %>%
  pull(b_i)

rmse.movie <- RMSE(d.test$rating, predicted.b_i)

# Model 03: Adding user effect (b_u)
avg.users <- d.train %>%
  left_join(avg.movies, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu_hat - b_i))

predicted.b_u <- d.test %>%
  left_join(avg.movies, by="movieId") %>%
  left_join(avg.users, by="userId") %>%
  mutate(pred = mu_hat + b_i + b_u) %>%
  pull(pred)

rmse.user <- RMSE(predicted.b_u, d.test$rating)

# Model 04: Adding genre combination effect (b_g)
avg.genres <- d.train %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  group_by(genres) %>%
  summarise(b_g = mean(rating - mu_hat - b_i - b_u))

predicted.b_g <- d.test %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  left_join(avg.genres, by = "genres") %>%
  mutate(pred = mu_hat + b_i + b_u + b_g) %>%
  pull(pred)

rmse.genre <- RMSE(predicted.b_g, d.test$rating)

# Model 05: Adding movie release year effect (b_y)
avg.years <- d.train %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  left_join(avg.genres, by = "genres") %>%
  group_by(year) %>%
  summarise(b_y = mean(rating - mu_hat - b_i - b_u - b_g))

predicted.b_y <- d.test %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  left_join(avg.genres, by = "genres") %>%
  left_join(avg.years, by = "year") %>%
  mutate(pred = mu_hat + b_i + b_u + b_g + b_y) %>%
  pull(pred)

rmse.year <- RMSE(predicted.b_y, d.test$rating)

# Model 06: Adding review delay effect (b_d)
avg.delays <- d.train %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  left_join(avg.genres, by = "genres") %>%
  left_join(avg.years, by="year") %>%
  group_by(reviewDelay) %>%
  summarise(b_d = mean(rating - mu_hat - b_i - b_u - b_g))

predicted.b_d <- d.test %>%
  left_join(avg.movies, by = "movieId") %>%
  left_join(avg.users, by = "userId") %>%
  left_join(avg.genres, by = "genres") %>%
  left_join(avg.years, by = "year") %>%
  left_join(avg.delays, by = "reviewDelay") %>%
  mutate(pred = mu_hat + b_i + b_u + b_g + b_y + b_d) %>%
  pull(pred)

rmse.delay <- RMSE(predicted.b_d, d.test$rating)

# Model 07: Regularization of models.

# Generate a sequence of values for lambda ranging from 4 to 6 with 0.05 increments
inc <- 0.05
lambdas <- seq(4, 6, inc)

rmses <- sapply(lambdas, function(l){
  b_i <- d.train %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu_hat)/(n()+l))
  b_u <- d.train %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu_hat)/(n()+l))
  b_g <- d.train %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarise(b_g = sum(rating - b_i - b_u - mu_hat)/(n()+l))
  b_y <- d.train %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(year) %>%
    summarise(b_y = sum(rating - b_i - b_u - b_g - mu_hat)/(n()+l))
  b_d <- d.train %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_y, by="year") %>%
    group_by(reviewDelay) %>%
    summarise(b_d = sum(rating - b_i - b_u - b_g - mu_hat)/(n()+l))
  predicted_ratings <- d.test %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_y, by="year") %>%
    left_join(b_d, by="reviewDelay") %>%
    mutate(pred = mu_hat + b_i + b_u + b_g + b_y + b_d) %>%
    pull(pred)
  return(RMSE(predicted_ratings, d.test$rating))
})

# Get best Lambda and RMSE for regularized model
lambda <- lambdas[which.min(rmses)]
rmse.regularized <- min(rmses) 

# Plot Lambdas vs RMSE
d <- as.data.frame(lambdas)
d$rmses <- rmses
names(d) <- c("lambdas", "rmses")
```

The evaluation metric utilized in the data modelling of this research is the Root Mean Square Error. The objective of this project was to achieve a RMSE lower than 0.86490.

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}\left(\hat{y}_{u,i}-y_{u,i}\right)^2}$$ 

The modelling was made on the edx dataset and not the validation (hold-out) dataset. This was split into training and testing datasets at 80%:20% ratios respectively. A total of seven models were created in an incremental fashion as shown in Table \@ref(tab:models). The second model (Movie) uses the Naive Prediction based on mean rating average and adds compensates for the movie effect. The subsequent, builds on model 2 and compensates for the user effect, until we have model 7 were regularization is used to compensate for the disparity in movie ratings, considering the mean rating and compensating for movie, user, genre combination, movie release year and review delay effect.

```{r models}
rmse.results %>% select(Model)%>%
  kable(col.names = c("Model"),
        caption = "Models",
        align = "lrr", booktabs = TRUE, format = "latex", linesep = "")
```

When regularlization was applied to the algorithm a number of lambdas were considered and the one yielding the lowest RMSE was chosen. With the final formula identified the model was applied to the hold-out dataset.

# **Results**

For regularization a number of lambdas were considered as shown in Figure \@ref(fig:lambdas). The best lambda was `r lambda` and a minimum RMSE of `r rmse.regularized` was obtained on the edx dataset. The RMSE values of each model are provided in Table \@ref(tab:RMSEs).

```{r lambdas, fig.cap="Lambdas vs RMSE evaluation"}
ggplot(d, aes(lambdas, rmses)) +
  geom_point() +xlab("Lambda") + ylab("RMSE")+
  geom_label_repel(data=subset(d, lambdas ==lambda),aes(label = lambdas), color = 'blue',
                   size = 3.5, box.padding = unit(0.75, "lines"),
                   point.padding = unit(0.3, "lines"))
```

```{r RMSEs}
rmse.results %>% 
  kable(col.names = c("Model", "RMSE"),
        caption = "Model evaluations",
        align = "lrr", booktabs = TRUE, format = "latex", linesep = "")
```

Prior to running the model on the hold-out dataset, the validation data needed to be transformed to match the changes done to the edx dataset. Namely:

 1. Converting the review date
 2. Extracting the movie release year
 3. Calculating the review delay.
 
After applying the regularized model with all predictors a RMSE of `r rmse.valid` was achieved, meeting the goal of this project.

```{r modelEvaluation, eval=FALSE}
# Convert timestamp into date format, removing time data
validation <- validation %>% mutate(reviewDate = round_date(as_datetime(timestamp), unit = "week"))

# Extraction of release year
validation <- validation %>% mutate(title = str_trim(title)) %>%
  extract(title, c("shortTitle", "year"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = FALSE) %>%
  mutate(year = as.integer(year)) %>%
  select(-shortTitle)

# Identification of reviews submitted prior to movie release year
validation <- validation %>%
  mutate(reviewDelay = year(reviewDate)-year)

##########################################################
# Data modelling on validation
##########################################################
# Regularized model on mean with movie, user, genre combination, release year and review delay effects.
b_i <- edx %>%
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu_hat)/(n()+lambda))
b_u <- edx %>%
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu_hat)/(n()+lambda))
b_g <- edx %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  group_by(genres) %>%
  summarise(b_g = sum(rating - b_i - b_u - mu_hat)/(n()+lambda))
b_y <- edx %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  group_by(year) %>%
  summarise(b_y = sum(rating - b_i - b_u - b_g - mu_hat)/(n()+lambda))
b_d <- edx %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  left_join(b_y, by="year") %>%
  group_by(reviewDelay) %>%
  summarise(b_d = sum(rating - b_i - b_u - b_g - mu_hat)/(n()+lambda))
predicted.final <- validation %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  left_join(b_y, by="year") %>%
  left_join(b_d, by="reviewDelay") %>%
  mutate(pred = mu_hat + b_i + b_u + b_g + b_y + b_d) %>%
  pull(pred)

##########################################################
# Results on validation
##########################################################
rmse.valid <- RMSE(validation$rating, predicted.final)
```

# **Conclusion**

In this research a recommender system was created for the 10 million movielens dataset with a RMSE of `r rmse.valid` when tested on a hold-out dataset, also known as the validation dataset which is 10% of the original dataset. A total of seven models were considered on the main dataset with the final and best model using regularization of the mean rating whilst compensating for the movie, user, genre combination, movie release year and review delay effects. The best RMSE obtained during training was of `r rmse.regularized`.

It is possible to further improve on this model by considering the individual genre bias rather than the combination and other recommender models such as collaborative filtering. It is also recommended to explore different training approaches rather than just a train:test data split as applied here, such as cross validation.